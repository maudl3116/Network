---
title: "Network structure from rich but noisy data"
author: Natalia Garcia Martin & Maud Lemercier
bibliography: references.bib
date: "`r Sys.Date()`"
output: 
  rmarkdown::pdf_document:
      citation_package: natbib
      extra_dependencies:
        stmaryrd: null
        color: null
        algorithm2e: ["ruled", "vlined"]
        pseudocode: null
      number_sections: true
      fig_caption: yes
      keep_tex: yes
header-includes: \usepackage{float} 
vignette: >
  %\VignetteIndexEntry{Vignette Title}
  %\VignetteEngine{knitr::rmarkdown}
  \usepackage[utf8]{inputenc}
abstract: "We have implemented an R package to estimate the parameters of exponential random graph models (ERG models), in the case where the data consists of noisy observations of the underlying hidden structure. The first part of this report focusses on the Bernouilli (or Erdos Renyi) model, performing inference via the Expectation Maximization algorithm, while the second part explores other ERG models, for which Bayesian parameter estimation is more challenging."  
---
# Introduction
## Problem formulation and motivation
The true network structure is drawn from $P(A|\theta_A)$. We will later constrain this distribution to belong to the exponential random graph models family. The observations of the network are supposed to be noisy. The network structure and the observations are related to one another by $P(\mbox{data}|A,\theta_Y)$. Our aim is to infer the parameters $\theta=\{\theta_A,\theta_Y\}$ through the posterior distribution $P(\theta|\mbox{data})$. $$P(\theta|\mbox{data})\propto \sum_AP(\mbox{data}|A,\theta_Y)P(A|\theta_A)P(\theta)$$ This optimisation problem is hard since the function is often non convex, might have many local maxima and no analytical solutions. In our model, as illustrated on Figure \ref{fig:graph_model}, we have two unknowns: the latent variables and the parameters. We also suppose that the underlying network is probed $k$ times.  

```{r pressure, echo=FALSE, fig.pos="H", out.width = '65%',fig.cap="\\label{fig:graph_model}Graphical Model"}
knitr::include_graphics("GM2.png")

```

In this report, we consider unweighted undirected networks, characterised by an $n \times n$ symmetric adjacency matrix $A$, having elements $A_{i,j} = 1$ if nodes $i$ and $j$ are connected by an edge and $0$ otherwise. Similarly, we note $Y_{i,j}^{(k)}$ the edge measurement between node $i$ and $j$ at time $k$.

## Network models

### Bernoulli (Erdős–Rényi) model

The Bernoulli or Erdős–Rényi model is one of the simplest models for random networks. Starting with $n$ isolated vertices, for any pair of nodes $(i, j)$ an edge is added with probability $\rho>0$. This step is repeated for the $\binom{n}{2}=n(n-1)/2$ pairs of nodes. This network is determined by the number of nodes $n$, the probability of edge formation $\rho$ and the total number edges $m$ and can be expressed as either $G(n,\rho)$ or $G(n,m)$. We will use the former notation. Since there are $\binom{n}{2}$ pairs of nodes in the network, the expected number of edges is given by $\binom{n}{2}\rho$. Similarly, the expected number of 2-stars is $\binom{n}{3}\rho^2$ and the expected number of triangles is $\binom{n}{3}\rho^3$. The degree $deg(j)$ of a node $j$ is defined as the number of edges connected to it. Given the adjacency matrix $A$, this is
$$ deg(j)=\sum_{i}{A_{i,j}}.$$

The average degree of the graph is then given by the average of the individual nodes degrees. For a Bernoulli graph, the probability that a given node has degree $d$ is the product of $a)$ the probability that $d$ links are present, $b)$ the probability that the remaining potential links are missing, and $c)$ the number of combinations in which we can select the $d$ links out of the potential $n-1$. Hence, they follow a Binomial distribution:

$$ P(deg(j)=d)=p_d=\binom{n-1}{d}\rho^d(1-\rho)^{n-1-d} $$

with expected degree $\rho(n-1)$. For large $n$ and small $\rho$, this distribution can be approximated by a Poisson distribution with parameter $\rho(n-1)$. Due to all pairs of nodes having the same probability of edge formation, the Erdős–Rényi model is not appropriate to represent social networks, where edges tend to form between nodes which already share common connections.\


### ERGM model

Exponential random graph models (ERGM) are a family of probability distributions on graphs. Let $\mathcal{A}_n$ be the set of all graphs on $n$ vertices, and consider the following model, where $\theta_i,~i=1,2,...,m$ are real valued parameters, and $T_i,~i=1,2,...,m$ are real valued statistics defined on $\mathcal{A}_n$. Possible sufficient statistics for such models are the degree of the vertices, the number of edges, the number of k-stars, the number of triangles, or the number of connected components. Altering these features allows us to construct networks which resemble social relationships. For example, triadic closure is a common occurrence in friendship networks, where as explained before, nodes with common neighbours are prone to form ties between them, resulting in a large number of triangles. The general form for an ERGM is

$$
\begin{aligned}
P(\mbox{A}=a|\theta)&=\frac{1}{Z(\theta)}\exp\left(\sum_{i=1}^m{\theta_iT_i(a)}\right) \\ &=\frac{1}{Z(\theta)}\exp\left[\theta^tT(a)\right],
\end{aligned}
$$

where $Z(\theta)$ is a normalisation constant satisfying $Z(\theta)=\sum_{a \in \mathcal{A}_n}\exp\left[\theta^tT(a)\right]$ and $\theta,T(a) \in \mathbb{R}^m$ represent the vector of parameters and the vector of sufficient statistics respectively. The 1-dimensional case with $\theta = \frac{\rho}{1-\rho}$ and $T(a)=\sum_{i,j}{a_{ij}}$ gives the expression for the Bernoulli graph. \

```{r echo=FALSE, fig.align='center', fig.pos="H", out.width = '60%',out.height='60%',fig.cap="\\label{fig:triangle}Example of edge, 2-star, 3-star and triangle"}
knitr::include_graphics("triangle.png")

```
# Methods
## Inference for the Bernoulli model

Since the underlying matrix $A$ is symmetric, we consider the lower triangular part of the matrix only and simulate the interactions for each day $k$ as edge observations $Y_{i,j}^{(k)}$ for entries $i<j$. These observations are assumed to be independent Bernoulli random variables conditioned on $A_{i,j}$. Given that the prior probability for a random edge is $\rho$, the prior model for the network is 
$$P(A|\rho)=\prod_{i<j}\rho^{A_{i,j}}(1-\rho)^{1-A_{i,j}}.$$

The data model is specified by $P(\mbox{data}|A, \theta)$, where $data$ corresponds to the observed $Y_{i,j}^{(k)}$. To simplify the notations, we let $E_{i,j}=\sum_kY_{i,j}^{(k)}$. Applying Bayes' theorem,
$$P(A,\theta|\mbox{data})=\frac{P(\mbox{data}|A,\theta)P(A)P(\theta)}{P(\mbox{data})}.$$

Our aim is to find the parameters $\theta$ that fix the relationship between the network and the data in order to then estimate the actual network structure. Summing over all the possible network structures $A$, we obtain the likelihood function
$$P(\theta|\mbox{data})=\sum_{A}{P(A,\theta|\mbox{data})}$$

which we want to maximise to find the best estimates of $\theta$ given the observed data. Taking logarithms in both sides and making use of the Jensen inequality, we get
$$\begin{aligned}
\log P(\theta|\mbox{data}) &= \log \sum_{A}{P(A,\theta|\mbox{data})} \\
                          &= \log \sum_{A} q(A)\frac{P(A,\theta|\mbox{data})}{q(A)} \\
                          &\geq \sum_{A} q(A) \log \frac{P(A,\theta|\mbox{data})}{q(A)},
\end{aligned}$$

where $q(A)$ is any probability distribution over networks satisfying $\sum_{A}q(A)=1.$ The right hand size of the equation is maximised in the case of equality, that is, when 
$$q(A)=\frac{P(A,\theta|\mbox{data})}{ \sum_{A}P(A,\theta|\mbox{data})}.$$

Notice that $q(A)=P(A|\mbox{data}, \theta)$. We will use this result in order to estimate the parameters $\theta$ and the true underlying network $A$  by employing an EM algorithm. We now define the true-positive rate $\alpha$ and the false-positive rate $\beta$ respectively as the probabilities of observing an edge when it actually exists and observing it when it does not exist. We assume that the priors for $\alpha$, $\beta$ and $\rho$ are uniform in the interval [0,1]. Then,


$$\begin{aligned}
P(\mbox{data}|A, \theta)
&=\prod_{i<j}\left(\alpha^{\sum_{k}Y_{i,j}^{(k)}}(1-\alpha)^{N_{i,j}-\sum_kY_{i,j}^{(k)}}\right)^{A_{i,j}}\left(\beta^{\sum_kY_{i,j}^{(k)}}(1-\beta)^{N_{i,j}-\sum_kY_{i,j}^{(k)}}\right)^{1-A_{i,j}} \\
&=\prod_{i<j}\left(\alpha^{E_{i,j}}(1-\alpha)^{N_{i,j}-E_{i,j}}\right)^{A_{i,j}}\left(\beta^{E_{i,j}}(1-\beta)^{N_{i,j}-E_{i,j}}\right)^{1-A_{i,j}}.
\end{aligned}$$

Combining this equation with the prior model for the network involving $\rho$ that we defined above, we obtain
$$\begin{aligned}
P(A,\theta|\mbox{data})
&=\frac{1}{P(\mbox{data})}\prod_{i<j}\left(\alpha^{E_{i,j}}(1-\alpha)^{N_{i,j}-E_{i,j}}\right)^{A_{i,j}}\left(\beta^{E_{i,j}}(1-\beta)^{N_{i,j}-E_{i,j}}\right)^{1-A_{i,j}} p(A)p(\theta) \\
&=\frac{1}{P(\mbox{data})}\prod_{i<j}\left(\rho \alpha^{E_{i,j}}(1-\alpha)^{N_{i,j}-E_{i,j}}\right)^{A_{i,j}}\left((1-\rho)\beta^{E_{i,j}}(1-\beta)^{N_{i,j}-E_{i,j}}\right)^{1-A_{i,j}}.
\end{aligned}$$

## The EM (expectation-maximization) algorithm{-}

**E-step**

The expectation step consists in updating the posterior probabilities with the previous estimates of the parameters. 

$$q^{(t)}(A)=\frac{p(A,\hat{\theta}^{(t)}|\mbox{data})}{\sum_A{p(A,\hat{\theta}^{(t)}|\mbox{data})}}$$
**M-step**

The maximisation step consists in solving the tractable optimisation problem: 
$$\sum_Aq^{(t)}(A)\nabla_\theta \log~{p(A,\hat{\theta}^{(t+1)}|\mbox{data})}=0$$
With the Bernoulli model, the maximisation step is tractable, since we have: 
$$\begin{split}P(A,\theta|\mbox{data})&=\frac{1}{P(\mbox{data})}\prod_{i<j}\left(\rho \alpha^{E_{i,j}}(1-\alpha)^{N_{i,j}-E_{i,j}}\right)^{A_{i,j}}\left((1-\rho)\beta^{E_{i,j}}(1-\beta)^{N_{i,j}-E_{i,j}}\right)^{1-A_{i,j}}\end{split}.$$

To simplify the notations, let $Q_{i,j}=P(A_{i,j}=1|data,\theta)=\sum_{A}{q(A)A_{i,j}}.$

$$\begin{pseudocode}[shadowbox]{EM}{\alpha^{(0)},\beta^{(0)}, \rho^{(0)}}
(\alpha,\beta, \rho) \leftarrow (\alpha^{(0)},\beta^{(0)}, \rho^{(0)}) \\
\\ 
\WHILE \mbox{stopping criterion is not met} \DO 
\BEGIN
\mbox{E step:}\\ 
~~~Q_{i,j} \leftarrow \frac{\rho\alpha^{E_{i,j}}(1-\alpha)^{N-E_{i,j}}}{\rho\alpha^{E_{i,j}}(1-\alpha)^{N-E_{i,j}}+(1-\rho)\beta^{E_{i,j}}(1-\beta)^{N-E_{i,j}}}\\
\mbox{M step:} \\
\BEGIN
~~~\alpha \leftarrow \frac{\sum_{i<j}E_{i,j}Q_{i,j}}{N\sum_{i<j}Q_{i,j}} \\
~~~\beta \leftarrow \frac{\sum_{i<j}E_{i,j}(1-Q_{i,j})}{N\sum_{i<j}(1-Q_{i,j})} \\
~~~\rho \leftarrow \frac{1}{{n \choose 2}}\sum_{i<j}Q_{i,j}
\END
\END \\ 
\RETURN{Q,\alpha,\beta, \rho}
\end{pseudocode}$$

## Inference for the ERGM model
The aim of this section is to discuss how a bayesian estimation of the parameters $\theta=\{\theta_A, \theta_Y\}$ can be performed. In our case, we cannot use a vanilla Metropolis Hastings algorithm with $p(\theta|Y)$ as a target distribution, for the following reasons:

* The normalizing function $Z(\theta)$ is intractable, and since it depends on $\theta$ it does not cancel out in the acceptance probability. This problem is would also arise in the case where the network is completely observed. 

* The likelihood appearing in the acceptance ratio is intractable, since we would have to sum over all the states that the hidden variable $A$ can take ($2^{n(n-1)/2}$ configurations since the $n(n-1)/2$ edges can be in two states).

$$
\begin{split}\alpha_{MH}(\theta,\theta') &= min\left\{1,\frac{p(y|\theta')p(\theta')q(\theta|\theta')}{p(y|\theta)p(\theta)q(\theta'|\theta)}\right\} \\ &= min\left\{1,\frac{\sum_Ap(y|\theta',A)p(A|\theta')p(\theta')q(\theta|\theta')}{\sum_Ap(y|\theta,A)p(A|\theta)p(\theta)q(\theta'|\theta)}\right\} \end{split}
$$

For this report, we have decided to restrict our study to the case where the observations are supposed to be drawn from an ERGM probability distribution, without modelling any noise. In this settings we use the following notation: $x$ a random variable drawn from an ERGM model with unnormalized probability model $h(x|\theta)=\exp(\theta^TT(x))$ and normalizing function $Z(\theta)$. Generalizing this model to the case of noisy observations is out of scope for this report.

## Maximum Pseudolikelihood estimation (MPML){-}
Recall the general form of the ERGM:
$$
P(\mbox{A}=a|\theta)=\frac{1}{Z(\theta)}\exp\left[\theta^tT(a)\right].$$

Maximum likelihood estimation involves the computation of the normalisation constant $Z(\theta)$, which is extremely hard to evaluate given the large number of possible structures of this network. Holland and Leinhardt (1981) were the first to develop a log-linear model for network data and tacked this problem by assuming independence in the dyads $(a_{ij}, a_{ji})$, which allows parameter estimation through MLE. One of the proposed methods for estimating parameters in our ERGM network is maximum pseudolikelihood estimation (Strauss and Ikeda, 1990). The pseudolikelihood function is given by the product of the probabilities of the $a_{ij}$, each of these conditioned on the rest of the data. We use the notation $a_{ij}=1$ to indicate the existence of an edge between nodes $i$ and $j$ and $a_{ij}=0$ if they are not connected. We define  $a_{ij}^c$ to be the status of all pairs in $a$ other than $(i,j)$. We also define $a_{ij}^+$ to be the same network as $a$ in the case where $a_{ij}=1$ and $a_{ij}^-$ for $a_{ij}=0$. Conditioned on $A_ij^c=a_ij^c$, A has only two possibilities: $A_{ij}=1$ or $A_{ij}=0$. We approximate the marginal $P(A_{ij}=1)$ by the conditional probability $P(A_{ij}=1|A_{ij}=a_{ij})$. The joint distribution can then be approximated as
$$
\begin{aligned}
P(A|\theta)&\approx \prod_{ij}P(a_{ij}|a_{ij}^c).
\end{aligned}
$$

The conditional odds are given by
$$\frac{P(a_{ij}=1|a_{ij}^c)}{P(a_{ij}=0|a_{ij}^c)}=\exp \left[\theta^t\left(T(a_{ij}^+)-T(a_{ij}^-)\right)\right].$$

Therefore,
$$\mbox{logit } P(a_{ij}=1|a_{ij}^c)= \theta^t\left[T(a_{ij}^+)-T(a_{ij}^-)\right]=\theta^t \Delta T_{ij},$$

where $\Delta T_{ij}=T(a_{ij}^+)-T(a_{ij}^-)$ is the vector of changes in T(a) when the entry $a_{ij}$ changes from 1 to 0. These conditional probabilities do not involve $Z(\theta)$ so we can maximise the pseudolikelihood function using logistic regression. However, it is hard to quantify the uncertainty around these estimates.

## Markov Chain Monte Carlo Maximum Likelihood estimation (MCMC-MLE){-}
Snijders (2002) introduces a method for estimating $\theta$ based on a Markov Chain Monte Carlo (MCMC) approximation of the MLE where the log-likelihood function is not evaluated directly. Instead, we take log ratios of the likelihood under proposed parameters $\theta$ and initial value of parameters $\theta_0$. We start by re-centering $P(\mbox{A}=a|\theta)=\frac{1}{Z(\theta)}\exp\left[\theta^tT(a)\right]$ around some value $a$ for which $T(a)=0$. Then, the log-likelihood can then be written as:
$$
\begin{aligned}
\mbox{loglik }\theta=\log L(\theta|a) 
&=\log\exp\left[\theta^tT(a)\right]- \log Z(\theta) \\
&=\theta^tT(a)- \log Z(\theta)\\
&=- \log Z(\theta).
\end{aligned}
$$

Therefore, taking log ratios and using the law of large numbers, we get
$$
\begin{aligned}
\mbox{loglik }\theta - \mbox{loglik }\theta_0 
&= - \log Z(\theta) + \log Z(\theta_0)\\
&= - \log \frac{Z(\theta)}{Z(\theta_0)}\\
&= - \log E _{\theta_0}\left[ \exp \left((\theta-\theta_0)^tT(a)\right) \right]\\
& \approx - \log \left( \frac{1}{L} \sum ^L_{i=1} \exp \left[(\theta-\theta_0)^tT(a^{(i)})\right] \right),
\end{aligned}
$$

where $a^{(1)}\dots a^{(L)}$ are a random sample of ERGM networks with parameter vector $\theta_0$. By differentiating on both sides, we obtain an approximate score function which can be iteratively approximately optimised using Newton-Raphson or Fisher scoring:
$$s(\theta)\approx - \frac{\partial}{\partial\theta} \log \left( \frac{1}{L} \sum ^L_{i=1} \exp \left[(\theta-\theta_0)^tT(a^{(i)})\right] \right).$$

To simulate the $L$ networks, we use the Metropolis Hastings algorithm provided by Snijders. We start with a graph $a^{(0)} \in \mathcal{A}_n$ —the set of all graphs on $n$ nodes— and proceed as follows for $k \in \{0\dots~L-1\}$:

\begin{algorithm}[H]
 Given $a_{ij}$ at the $k^{th}$ iteration \\
 1. Choose an edge $(i,j)$ at random from $a^{(k)}$ where $i\neq j$ \\ 
 2. Calculate $\pi =\frac{P(a_{ij} \mbox{~changes } |a_{ij}^c)}{P(a_{ij} \mbox{~does not change }|a_{ij}^c)}$ \\ 
 3. Fix $\delta=\mbox{min}\{1, \pi\}$ and draw $u$ from Bin$(1,\delta)$. \\
 4. If $u=0$, set $a^{(k+1)}=a^{(k)}.$ If $u=1$, set $a_{pq}^{(k+1)}=a_{pq}^{(k)}$ for $(p,q)\neq(i,j)$ and $a_{pq}^{(k+1)}=1-a_{pq}^{(k)}$ for $(p,q)=(i,j)$. \\
 \caption{Metropolis Hastings for MCMCMLE}
\end{algorithm}


## The Exchange Algorithm{-}
The exchange algorithm (\cite{murray2012mcmc}) falls within the class of *auxiliary variable approaches*, which consist in introducing an auxiliary variable $u$ with a well-chosen conditional density $f(u|\theta,x)$ so that the intractable normalizing function cancels out in the Metropolis-Hastings acceptance probability. In the case of the exchange algorithm, this conditional density is $\frac{h(u|\theta)}{Z(\theta)}$.

The exchange algorithm augments the state from $\theta$ to $(\theta, \theta', u)$ and targets the augmented joint density

$$\begin{split}\pi(\theta,\theta',u|x) & \propto p(\theta)L(\theta|x)q(\theta'|\theta)L(\theta'|u) \\ & = p(\theta)\frac{h(x|\theta)}{Z(\theta)}q(\theta'|\theta)\frac{h(u|\theta')}{Z(\theta')}\end{split}$$.
with a symmetric swapping proposal between the states $\{\theta',\theta\}$ and $\{\theta,\theta'\}$. Therefore the acceptance probability is 

$$\alpha=min \left\{1,\frac{\pi(\theta',\theta,u|x)}{\pi(\theta,\theta',u|x)}\right\}=min\left\{1,\frac{p(\theta')h(x|\theta')h(u|\theta)q(\theta|\theta')}{p(\theta)h(x|\theta)h(u|\theta')q(\theta'|\theta)}\right\}$$

```{r echo=FALSE, fig.align='center', fig.pos="H", out.width = '20%',out.height='20%',fig.cap="\\label{fig:triangle}The augmented model"}
knitr::include_graphics("exchange.png")

```

\begin{algorithm}[H]
 Given $\theta_n \in \Theta$ at the $n^{th}$ iteration \\
 1. Propose $\theta' \sim q(\cdot|\theta_n)$ \\ 
 2. Generate the auxiliary variable $u \sim \frac{h(\cdot|\theta')}{Z(\theta')}$ \\ 
 3. Accept $\theta_{n+1}$ with probability $\alpha = min\left\{1, \frac{p(\theta')h(x|\theta')h(u|\theta_n)q(\theta_n|\theta')}{p(\theta_n)h(x|\theta_n)h(u|\theta')q(\theta'|\theta_n)}\right\}$\\
 ~~~~Reject otherwise
 \caption{Exchange algorithm}
\end{algorithm}



## The Double Metropolis-Hastings algorithm{-}
The double Metropolis-Hastings algorithm uses a Metropolis-Hastings sampler to generate $\theta$ draws (outer sampler), and another one to to generate the auxiliary variables (inner sampler). Compared to the exchange algorithm, the auxiliary variable $u$ is now obtained after $m$-MH updates starting from $x$, under $\theta'$. Besides, since the inner sampler update $T_{\theta'}^{m}(u|x)$ satisfies the detailed balance condition

$$h(x|\theta')T_{\theta'}^m(u|x)=h(u|\theta')T_{\theta'}^m(x|u)$$

the acceptance probability of the DMH algorithm is 

$$\alpha = min\left\{1, \frac{p(\theta')T_{\theta'}^{m}(x|u)h(u|\theta_n)q(\theta_n|\theta')}{p(\theta_n)h(x|\theta_n)T_{\theta'}^{m}(u|x)q(\theta'|\theta_n)}\right\}$$
\begin{algorithm}[H]
 Given $\theta_n \in \Theta$ at the $n^{th}$ iteration \\
 1. Propose $\theta' \sim q(\cdot|\theta_n)$ \\ 
 2. Generate the auxiliary variable using $m$ MH-updates 
 ~~$u \sim T^m_{\theta'}(.|x)$ \\ 
3. Accept $\theta_{n+1}$ with probability $\alpha = min\left\{1, \frac{p(\theta')T^m_{\theta'}(x|u)h(u|\theta_n)q(\theta_n|\theta')}{p(\theta_n)h(x|\theta_n)T^m_{\theta'}(u|x)q(\theta'|\theta_n)}\right\}$\\
 ~~~~Reject otherwise 
 \caption{Double Metropolis-Hastings algorithm}
\end{algorithm}

# Results

## EM algorithm for noisy observations

**Dataset**

In this section, we propose to validate the model’s estimates using a synthetic dataset for which the ground truth network and the noisy observations are generated via a predefined probabilistic model. We simulate a network with `r n=100` $n=$ `r n` vertices, and we set $\rho=$ `r rho=0.1` `r rho`. The noisy observations are then simulated for $k=$ `r k=5` `r k` days, with true positive rate $\alpha=$ `r alpha=0.6` `r alpha` and false positive rate $\beta=$ `r beta=0.009` `r beta`. Our stopping criterion is met when the absolute value of the difference of all parameter values after an iteration is less than $\epsilon=$ `r epsilon=0.001` `r epsilon`. 

```{r echo=FALSE, results='hide',fig.keep='none',fig.pos="H"}
   # generate ground truth network
  output <- sampleErdosRenyi(n,rho)
  g <- output[[2]]
  A <- output[[1]]

  # generate noisy observations of the ground truth network
  E <- interact(A,alpha,beta, n,k)
  simulation <- EM(alpha0=0.4, beta0=0.02, rho0=0.15, n, k, E)
```
\newpage
**Results**

In this setting, the stopping criterion is met after `r simulation[5]` `r nIter=simulation[5]` iterations. The figure below, shows the comparison of the ground truth network with the inferred network (obtained by thresholding the posterior probabilities at $t=$ `r t=0.5` `r t`), where the size of the nodes is proportional to their degree.


```{r echo=FALSE, fig.pos="H",fig.cap="\\label{fig:comparison}(left) Ground truth underlying network (right) Inferred underlying network"}
  out=analyse_results(t,n,k,rho,alpha,beta)
```


```{r echo=FALSE,fig.pos="H",fig.height = 2,fig.width=8,fig.align = "center",fig.cap="\\label{fig:figs} Convergence of the parameter estimates"}

m1<-simulation[[6]][c(1:simulation[[5]]),]
par(mfrow = c(1, 3))
plot(m1[,1], type="l",xlab="iteration number",ylab=expression(alpha))
plot(m1[,2],type="l",xlab="iteration number",ylab=expression(beta))
plot(m1[,3],type="l",xlab="iteration number",ylab=expression(rho))
```



$$
\centering\begin{tabular}{|c|c|}\hline Precision & `r out$Precision` \\ \hline Recall & `r out$Recall` \\ \hline Accuracy & `r out$Accuracy` \\ \hline F-measure & `r out$F_measure` \\ \hline\end{tabular}$$

The figure below shows the influence of the number of repeated observations $k$ on the performances of the algorithm. 

```{r echo=FALSE,fig.pos="H",fig.height = 4,fig.width=10,fig.align = "center",fig.cap="\\label{fig:figs}Performance metrics versus the number of repeated observations k"}
par(mfrow=c(1,2))
F_measure_plot(20, n=100)
F_measure_networksize(n=100)
```

## Exchange algorithm
## Double Metropolis-Hastings algorithm

# Discussion

\nocite{
morris2008specification,
eagle2006reality,
everitt2012bayesian,
korber2018bayesian,
jin2013bayesian,
newman2018network,
koskinen2010analysing,
everitt2017marginal,
murray2012mcmc,
hunter2006,
park2018bayesian,
strauss1990pseudolikelihood,
snijders2002markov,
schmid2017exponential}
